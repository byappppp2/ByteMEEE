# -*- coding: utf-8 -*-
"""AMLRules.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5BjSZUHfDpE65ywHaRoA9h-edrGSSn4
"""

import os
import sys
import time
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

from fastapi import FastAPI, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
import json
from datetime import datetime, timedelta

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_curve, roc_curve, roc_auc_score



#FAST API

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# === FastAPI endpoint ===
@app.post("/process")
async def process_file(file: UploadFile = File(...)):
    # Read the uploaded CSV
    #df = pd.read_csv(file.file)

    # Run your fraud detection logic #
    scanVolume, flaggedRecords, proportionFlagged, topViolations = main(file)

    # Return JSON to frontend  # this is the one!!! to put in all the values
    return {
        "scanVolume": scanVolume,
        "flaggedRecords": flaggedRecords,
        "proportionFlagged": proportionFlagged,
        "topViolations": topViolations
    }

# === Run server ===
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=5000, reload=True)


# =========================================================
# CONFIG
# =========================================================
CSV_PATH = "HI-Small_Trans.csv"
FLAGGED_PATH = "flagged_transactions.csv"
DF_PATH = "transactions_scored.csv"

# Amount thresholds (refined)
HIGH_VALUE = 15_000                        # Lowered for better sensitivity
VERY_HIGH_VALUE = 50_000                   # New threshold for critical transactions
SUSPICIOUS_AMOUNT = 9_500                  # Just under reporting threshold

# Time windows (refined)
ODD_HOUR_START, ODD_HOUR_END = 0, 5        # Extended odd hours (00:00–05:59)
WEEKEND_HOURS = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]  # Weekend analysis
STRUCT_WINDOW_HOURS = 24                   # window for structuring per (from->to)
VELOCITY_WINDOW_HOURS = 24                 # outgoing velocity per originating account
DIVERSITY_WINDOW_DAYS = 1                  # distinct counterparties per day

# Counts / thresholds (refined)
STRUCT_MIN_COUNT = 3                       # Lowered for better detection
VELOCITY_MIN_COUNT = 15                    # Lowered threshold
COUNTERPARTY_DIVERSITY_MIN = 8             # Lowered threshold
RAPID_TRANSACTION_COUNT = 5                # New: rapid transactions in short time

# Plot toggle (optional)
MAKE_PLOT = True


def step(msg: str):
    print(f"\n{'=' * 56}\n▶ {msg}\n{'=' * 56}")


def safe_preview(df: pd.DataFrame, n=5, name="DataFrame"):
    try:
        from IPython.display import display  # type: ignore
        display(df.head(n))
    except Exception:
        print(f"\n{name} preview (top {n} rows):")
        print(df.head(n).to_string(index=False))


def normalize_currencies_to_usd(df: pd.DataFrame) -> pd.DataFrame:
   
    fallback_rates = {
        'EUR': 1.08, 'GBP': 1.27, 'JPY': 0.0067, 'CAD': 0.74,
        'AUD': 0.66, 'CHF': 1.12, 'CNY': 0.14, 'INR': 0.012,
        'BRL': 0.20, 'MXN': 0.059, 'KRW': 0.00076, 'SGD': 0.74,
        'HKD': 0.13, 'NZD': 0.61, 'SEK': 0.095, 'NOK': 0.095,
        'DKK': 0.14, 'PLN': 0.25, 'CZK': 0.044, 'HUF': 0.0028,
        'RUB': 0.011, 'TRY': 0.034, 'ZAR': 0.055, 'THB': 0.028,
        'MYR': 0.21, 'IDR': 0.000064, 'PHP': 0.018, 'VND': 0.000041
    }
    
    def get_exchange_rate(currency: str) -> float:
        currency = currency.upper().strip()
        if currency == 'USD':
            return 1.0

        try:
            url = "https://api.exchangerate-api.com/v4/latest/USD"
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                data = response.json()
                if currency in data.get('rates', {}):
                    return 1.0 / data['rates'][currency]
        except:
            pass
        
        return fallback_rates.get(currency, 1.0)
    
    df = df.copy()
    
    # Normalize amount_paid
    if 'amount_paid' in df.columns and 'payment_currency' in df.columns:
        unique_currencies = df['payment_currency'].dropna().unique()
        exchange_rates = {curr: get_exchange_rate(str(curr)) for curr in unique_currencies}
        
        def convert_paid_to_usd(row):
            amount = row['amount_paid']
            currency = row['payment_currency']
            if pd.isna(amount) or pd.isna(currency):
                return np.nan
            rate = exchange_rates.get(str(currency).upper(), 1.0)
            return float(amount) * rate
        
        df['amount_paid_usd'] = df.apply(convert_paid_to_usd, axis=1)
        print(f"✅ Converted {len(unique_currencies)} currencies for amount_paid")
    
    # Normalize amount_received
    if 'amount_received' in df.columns and 'receiving_currency' in df.columns:
        unique_currencies = df['receiving_currency'].dropna().unique()
        exchange_rates = {curr: get_exchange_rate(str(curr)) for curr in unique_currencies}
        
        def convert_received_to_usd(row):
            amount = row['amount_received']
            currency = row['receiving_currency']
            if pd.isna(amount) or pd.isna(currency):
                return np.nan
            rate = exchange_rates.get(str(currency).upper(), 1.0)
            return float(amount) * rate
        
        df['amount_received_usd'] = df.apply(convert_received_to_usd, axis=1)
        print(f"✅ Converted {len(unique_currencies)} currencies for amount_received")
    
    return df


def validate_file_security(file_path: str) -> bool:
    print(f"🔍 Validating file: {file_path}")
    
    if not os.path.exists(file_path):
        print(f"❌ File not found: {file_path}")
        return False
    
    # Check file extension
    allowed_extensions = {'.csv', '.xlsx', '.xls', '.json'}
    file_ext = os.path.splitext(file_path)[1].lower()
    if file_ext not in allowed_extensions:
        print(f"❌ Invalid file type: {file_ext}")
        return False
    
    file_size = os.path.getsize(file_path)
    print(f"✅ File validation passed ({file_size / 1024:.1f}KB)")
    return True


def main(FILE):
    t0 = time.time()

    # -------------------------
    # 1) LOAD & CLEAN
    # -------------------------
    step("STEP 1: Load & normalize columns")


    # Validate file security
    CSV_PATH = FILE
    if not validate_file_security(CSV_PATH):
        print(f"ERROR: File validation failed for: {CSV_PATH}", file=sys.stderr)
        sys.exit(1)

    raw = pd.read_csv(CSV_PATH)

    y = raw["Is Laundering"].astype(int) #for checking later


    raw.columns = [c.strip() for c in raw.columns]

    # Map first two "Account*" columns to from/to account
    cols = raw.columns.tolist()
    acct_like = [i for i, c in enumerate(cols) if c.replace(".", "").lower().startswith("account")]
    if len(acct_like) >= 2:
        cols[acct_like[0]] = "from_account"
        cols[acct_like[1]] = "to_account"
        raw.columns = cols

    # Standardize common headers if present
    name_map = {
        "Timestamp": "timestamp",
        "From Bank": "from_bank",
        "To Bank": "to_bank",
        "Amount Received": "amount_received",
        "Receiving Currency": "receiving_currency",
        "Amount Paid": "amount_paid",
        "Payment Currency": "payment_currency",
        "Payment Format": "payment_format",
        "Is Laundering": "is_laundering",   # not used here, but we won’t break if present
    }
    for k, v in name_map.items():
        if k in raw.columns:
            raw = raw.rename(columns={k: v})

    df = raw.sample(n=12000) # no extra copy

    # Ensure key columns exist
    needed_cols = [
        "timestamp","from_bank","to_bank","from_account","to_account",
        "amount_received","amount_paid","payment_currency","receiving_currency",
        "payment_format"
    ]
    for col in needed_cols:
        if col not in df.columns:
            df[col] = np.nan if col in ["amount_received", "amount_paid"] else ""

    # Parse timestamp & numeric amounts
    df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")

    for amt in ["amount_received", "amount_paid"]:
        s = df[amt].astype(str).str.replace(",", "", regex=False).str.strip()
        df[amt] = pd.to_numeric(s.replace({"": np.nan}), errors="coerce").astype("float32")

    # Use categories for low-cardinality text to save memory
    for c in ["from_bank","to_bank","payment_currency","receiving_currency","payment_format"]:
        if c in df.columns:
            df[c] = df[c].astype("category")

    # -------------------------
    # 1.5) CURRENCY NORMALIZATION
    # -------------------------
    step("STEP 1.5: Currency normalization to USD")
    df = normalize_currencies_to_usd(df)

    # Choose canonical amount column (prefer USD normalized)
    if "amount_paid_usd" in df.columns:
        amount_col = "amount_paid_usd"
    elif "amount_paid" in df.columns:
        amount_col = "amount_paid"
    elif "amount_received_usd" in df.columns:
        amount_col = "amount_received_usd"
    else:
        amount_col = "amount_received"
    
    amt = df[amount_col].fillna(0.0).astype("float32")

    print("Shape:", df.shape)
    print("Columns:", df.columns.tolist())
    safe_preview(df, name="Raw (normalized)")

    # -------------------------
    # 2) ENHANCED AML FLAGS (O(N), vectorized)
    # -------------------------
    step("STEP 2: Enhanced AML flags (vectorized)")

    # Basic amount flags (refined)
    df["flag_high_value"] = (amt > HIGH_VALUE)
    df["flag_very_high_value"] = (amt > VERY_HIGH_VALUE)
    df["flag_suspicious_amount"] = (amt > SUSPICIOUS_AMOUNT) & (amt <= HIGH_VALUE)
    
    # Bank and account flags
    df["flag_cross_bank"] = (df["from_bank"].astype(str).values != df["to_bank"].astype(str).values)
    df["flag_same_account"] = (df["from_account"].astype(str).values == df["to_account"].astype(str).values)
    
    # Enhanced time-based flags
    df["hour"] = df["timestamp"].dt.hour
    df["day_of_week"] = df["timestamp"].dt.dayofweek
    df["flag_odd_hours"] = df["hour"].between(ODD_HOUR_START, ODD_HOUR_END, inclusive="both")
    df["flag_weekend"] = df["day_of_week"].isin([5, 6])  # Saturday, Sunday
    
    # Currency flags
    if {"payment_currency","receiving_currency"}.issubset(df.columns):
        df["flag_cross_currency"] = (
            df["payment_currency"].astype(str).str.upper().values
            != df["receiving_currency"].astype(str).str.upper().values
        )
    else:
        df["flag_cross_currency"] = False

    # Enhanced amount patterns
    df["flag_round_amount"] = (np.isclose((amt % 1000), 0) | np.isclose((amt * 100) % 100, 0)).astype(bool)
    df["flag_exact_threshold"] = np.isclose(amt, HIGH_VALUE, rtol=0.01)  # Just under reporting threshold
    df["flag_micro_amount"] = (amt > 0) & (amt < 100)  # Micro transactions
    
    # Payment format flags
    if "payment_format" in df.columns:
        df["flag_cash_equivalent"] = df["payment_format"].astype(str).str.contains(
            "cash|prepaid|gift", case=False, na=False
        )
    else:
        df["flag_cash_equivalent"] = False

    # -------------------------
    # 3) TIME-WINDOW PATTERNS via downsampling (fast & scalable)
    # -------------------------
    step("STEP 3: Time-window patterns (structuring, velocity, diversity)")

    # Hour and Day buckets
    df["ts_hour"] = df["timestamp"].dt.floor("H")
    df["ts_day"]  = df["timestamp"].dt.floor("D")

    # Guard on timestamps
    have_time = df["ts_hour"].notna().any()
    if not have_time:
        # If no timestamps, skip time-based flags safely
        df["flag_structuring"] = False
        df["flag_velocity_outgoing"] = False
        df["flag_counterparty_diversity"] = False
    else:
        # --- 3a) Structuring (many small transfers to same counterparty in 24h)
        small = amt < HIGH_VALUE
        small_df = df.loc[small, ["from_account","to_account","ts_hour"]].dropna(subset=["ts_hour"])

        # Hourly counts per (from->to)
        hourly_ft = (
            small_df
            .groupby(["from_account","to_account","ts_hour"], sort=True)
            .size().rename("cnt").reset_index()
        )

        # Rolling 24h sum over hourly counts per pair
        hourly_ft = hourly_ft.sort_values(["from_account","to_account","ts_hour"]).set_index("ts_hour")
        rolling_counts = (
            hourly_ft
            .groupby(["from_account","to_account"])["cnt"]
            .rolling(f"{STRUCT_WINDOW_HOURS}H")
            .sum()
            .reset_index()
        )
        rolling_counts["struct_hit"] = rolling_counts["cnt"] >= STRUCT_MIN_COUNT

        # Merge back by hour bucket
        df = df.merge(
            rolling_counts[["from_account","to_account","ts_hour","struct_hit"]],
            on=["from_account","to_account","ts_hour"],
            how="left"
        )
        df["flag_structuring"] = df["struct_hit"].fillna(False).values
        df.drop(columns=["struct_hit"], inplace=True)

        # --- 3b) Velocity: many outgoing txns from same origin account within 24h
        hourly_from = (
            df.dropna(subset=["ts_hour"])
              .groupby(["from_account","ts_hour"], sort=True)
              .size().rename("out_cnt").reset_index()
        )
        hourly_from = hourly_from.sort_values(["from_account","ts_hour"]).set_index("ts_hour")
        vel_roll = (
            hourly_from
            .groupby(["from_account"])["out_cnt"]
            .rolling(f"{VELOCITY_WINDOW_HOURS}H").sum()
            .reset_index()
        )
        vel_roll["vel_hit"] = vel_roll["out_cnt"] >= VELOCITY_MIN_COUNT

        df = df.merge(
            vel_roll[["from_account","ts_hour","vel_hit"]],
            on=["from_account","ts_hour"],
            how="left"
        )
        df["flag_velocity_outgoing"] = df["vel_hit"].fillna(False).values
        df.drop(columns=["vel_hit"], inplace=True)

        # --- 3c) Counterparty diversity per day: many distinct to_accounts from same origin
        daily_div = (
            df.dropna(subset=["ts_day"])
              .groupby(["from_account","ts_day"])["to_account"]
              .nunique()
              .rename("uniq_to")
              .reset_index()
        )
        daily_div["div_hit"] = daily_div["uniq_to"] >= COUNTERPARTY_DIVERSITY_MIN

        df = df.merge(
            daily_div[["from_account","ts_day","div_hit"]],
            on=["from_account","ts_day"],
            how="left"
        )
        df["flag_counterparty_diversity"] = df["div_hit"].fillna(False).values
        df.drop(columns=["div_hit"], inplace=True)

        # --- 3d) Rapid transactions: many transactions in short time window
        rapid_window_minutes = 30  # 30-minute window for rapid transactions
        df["ts_minute"] = df["timestamp"].dt.floor(f"{rapid_window_minutes}min")
        
        rapid_counts = (
            df.dropna(subset=["ts_minute"])
              .groupby(["from_account", "ts_minute"], sort=True)
              .size().rename("rapid_cnt").reset_index()
        )
        rapid_counts["rapid_hit"] = rapid_counts["rapid_cnt"] >= RAPID_TRANSACTION_COUNT
        
        df = df.merge(
            rapid_counts[["from_account", "ts_minute", "rapid_hit"]],
            on=["from_account", "ts_minute"],
            how="left"
        )
        df["flag_rapid_transactions"] = df["rapid_hit"].fillna(False).values
        df.drop(columns=["rapid_hit"], inplace=True)

    # -------------------------
    # 4) ENHANCED ML SCORING & SEVERITY
    # -------------------------
    step("STEP 4: Enhanced ML scoring & severity")

    # Choose which flags to score (enhanced feature set)
    rule_cols = [
        "flag_high_value", "flag_very_high_value", "flag_suspicious_amount",
        "flag_cross_bank", "flag_same_account", "flag_odd_hours", "flag_weekend",
        "flag_cross_currency", "flag_round_amount", "flag_exact_threshold",
        "flag_micro_amount", "flag_cash_equivalent",
        "flag_structuring", "flag_velocity_outgoing", "flag_counterparty_diversity",
        "flag_rapid_transactions"
    ]

    # Prepare features for ML models
    X = df[rule_cols].fillna(0).astype(int)
    Y = y

    print(f"Training {len(rule_cols)} rule-based features on {len(X)} transactions")

    # 1) Enhanced Logistic Regression
    lr_model = LogisticRegression(
        max_iter=2000, 
        random_state=42, 
        class_weight='balanced',  # Handle class imbalance
        C=0.1  # Regularization
    )
    lr_model.fit(X, Y)
    lr_weights = {f: round(w, 3) for f, w in zip(rule_cols, lr_model.coef_[0])}
    print("Logistic Regression weights:")
    for flag, weight in lr_weights.items():
        print(f"  {flag}: {weight}")
    
    lr_proba = lr_model.predict_proba(X)[:, 1]

    # 2) Enhanced Random Forest
    print("\nTraining Random Forest...")
    rf_model = RandomForestClassifier(
        n_estimators=200, 
        random_state=42, 
        n_jobs=-1,
        class_weight='balanced',
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2
    )
    rf_model.fit(X, Y)
    rf_proba = rf_model.predict_proba(X)[:, 1]
    rf_importance = {f: round(w, 3) for f, w in zip(rule_cols, rf_model.feature_importances_)}
    print("Random Forest feature importance:")
    for flag, importance in rf_importance.items():
        print(f"  {flag}: {importance}")

    # 3) Enhanced Isolation Forest
    print("\nTraining Isolation Forest...")
    contamination_rate = max(0.005, min(0.1, Y.mean())) if Y.nunique() > 1 else 0.05
    if_model = IsolationForest(
        contamination=contamination_rate, 
        random_state=42, 
        n_jobs=-1,
        n_estimators=200,
        max_samples=0.8
    )
    if_model.fit(X)
    if_scores = if_model.score_samples(X)
    if_proba = 1 / (1 + np.exp(if_scores))  # Convert to probability-like scores

    # Conservative ensemble weighting to reduce false positives
    # Weight supervised models more heavily for better precision
    ensemble_weights = [0.4, 0.4, 0.2]  # LR, RF, IF (supervised models get higher weight)
    df["risk_prob"] = (ensemble_weights[0] * lr_proba + 
                      ensemble_weights[1] * rf_proba +
                      ensemble_weights[2] * if_proba)

    # Store individual model scores
    df["lr_risk_score"] = lr_proba
    df["rf_risk_score"] = rf_proba
    df["if_anomaly_score"] = -if_scores  # Higher scores = more anomalous

    # Conservative severity classification targeting ~0.5% flagging rate
    def assign_severity(score):
        if score >= 0.95:
            return 'Critical'
        elif score >= 0.85:
            return 'High'
        elif score >= 0.75:
            return 'Medium'
        elif score >= 0.65:
            return 'Elevated'
        else:
            return 'Low'

    # Apply severity classification
    df["severity"] = df["risk_prob"].apply(assign_severity)
    
    # Conservative risk factors for severity adjustment
    df["risk_multiplier"] = 1.0
    
    # Increase risk for transactions with multiple flags (reduced impact)
    flag_count = df[rule_cols].sum(axis=1)
    df["risk_multiplier"] += flag_count * 0.02  # 2% increase per flag (reduced from 5%)
    
    # Increase risk for very high value transactions (reduced impact)
    df.loc[df["flag_very_high_value"], "risk_multiplier"] *= 1.2  # Reduced from 1.5
    
    # Increase risk for weekend transactions (reduced impact)
    df.loc[df["flag_weekend"], "risk_multiplier"] *= 1.1  # Reduced from 1.2
    
    # Apply risk multiplier to final score
    df["risk_prob_adjusted"] = df["risk_prob"] * df["risk_multiplier"]
    df["risk_prob_adjusted"] = np.clip(df["risk_prob_adjusted"], 0, 1)  # Keep in [0,1] range
    
    # Re-assign severity with adjusted scores
    df["severity"] = df["risk_prob_adjusted"].apply(assign_severity)
    
    # Additional percentile-based flagging to target ~0.5% flagging rate
    # Only flag top 0.5% of risk scores
    risk_threshold_95th = df["risk_prob_adjusted"].quantile(0.995)  # Top 0.5%
    risk_threshold_99th = df["risk_prob_adjusted"].quantile(0.999)  # Top 0.1%
    
    print(f"\nRisk Score Thresholds:")
    print(f"  95th percentile (0.5% flagging): {risk_threshold_95th:.4f}")
    print(f"  99th percentile (0.1% flagging): {risk_threshold_99th:.4f}")
    
    # Apply percentile-based severity override
    def assign_severity_percentile(score):
        if score >= risk_threshold_99th:
            return 'Critical'
        elif score >= risk_threshold_95th:
            return 'High'
        else:
            return 'Low'
    
    # Override severity for top percentiles
    df.loc[df["risk_prob_adjusted"] >= risk_threshold_99th, "severity"] = 'Critical'
    df.loc[(df["risk_prob_adjusted"] >= risk_threshold_95th) & 
           (df["risk_prob_adjusted"] < risk_threshold_99th), "severity"] = 'High'
    df.loc[df["risk_prob_adjusted"] < risk_threshold_95th, "severity"] = 'Low'

    print(df["severity"].value_counts(normalize=True))


    step("STEP 5: Save outputs & enhanced visualization")

    summary = (df["severity"].value_counts(dropna=False)
               .rename_axis("Severity")
               .reset_index(name="Count")
               .sort_values("Severity", key=lambda s: s.astype(str)))

    print("\nRisk Distribution Summary:")
    total_transactions = len(df)
    for _, row in summary.iterrows():
        severity = row["Severity"]
        count = row["Count"]
        percentage = (count / total_transactions) * 100
        print(f"  {severity:>10}: {count:>8,} transactions ({percentage:5.1f}%)")

    print(f"\nRisk Score Statistics:")
    print(f"  Average risk score: {df['risk_prob'].mean():.4f}")
    print(f"  Maximum risk score: {df['risk_prob'].max():.4f}")
    print(f"  Minimum risk score: {df['risk_prob'].min():.4f}")

    if MAKE_PLOT:
        try:
            # Create enhanced risk distribution chart
            plt.figure(figsize=(12, 8))
            
            # Main risk distribution chart
            plt.subplot(2, 2, 1)
            colors = ['#2E8B57', '#FFD700', '#FF6347', '#FF4500', '#DC143C']  # Green to Red
            bars = plt.bar(summary["Severity"].astype(str), summary["Count"], color=colors)
            plt.title("Transaction Risk Level Distribution", fontsize=14, fontweight='bold')
            plt.xlabel("Severity")
            plt.ylabel("Count")
            
            # Add value labels on bars
            for bar in bars:
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{int(height):,}', ha='center', va='bottom', fontsize=9)
            
            # Risk score distribution
            plt.subplot(2, 2, 2)
            plt.hist(df['risk_prob'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            plt.title("Risk Score Distribution", fontsize=14, fontweight='bold')
            plt.xlabel("Risk Score")
            plt.ylabel("Frequency")
            
            # Model comparison
            plt.subplot(2, 2, 3)
            model_scores = [df['lr_risk_score'].mean(), df['rf_risk_score'].mean(), df['if_anomaly_score'].mean()]
            model_names = ['Logistic\nRegression', 'Random\nForest', 'Isolation\nForest']
            plt.bar(model_names, model_scores, color=['lightblue', 'lightgreen', 'lightcoral'])
            plt.title("Average Scores by Model", fontsize=14, fontweight='bold')
            plt.ylabel("Average Score")
            
            # Add value labels on bars
            for i, (bar, score) in enumerate(zip(plt.gca().patches, model_scores)):
                plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + bar.get_height()*0.01,
                        f'{score:.3f}', ha='center', va='bottom', fontsize=9)
            
            # Top risky transactions
            plt.subplot(2, 2, 4)
            top_risky = df.nlargest(10, 'risk_prob')
            plt.scatter(range(len(top_risky)), top_risky['risk_prob'], 
                       c=top_risky['risk_prob'], cmap='Reds', s=50)
            plt.title("Top 10 Riskiest Transactions", fontsize=14, fontweight='bold')
            plt.xlabel("Transaction Rank")
            plt.ylabel("Risk Score")
            plt.colorbar(label='Risk Score')
            
            plt.tight_layout()
            plt.savefig('aml_risk_analysis.png', dpi=300, bbox_inches='tight')
            print("✅ Enhanced visualization saved as: aml_risk_analysis.png")
            plt.show()
            
        except Exception as e:
            print(f"(Plotting skipped: {e})")

    # Save full + flagged (only Critical and High risk for precision)
    flagged = df[df["severity"].isin(["Critical","High"])].copy()
    cols_to_show = [
        "timestamp","from_bank","from_account","to_bank","to_account",
        amount_col,"payment_currency","payment_format",
        "flag_high_value","flag_very_high_value","flag_suspicious_amount",
        "flag_cross_bank","flag_same_account","flag_odd_hours","flag_weekend",
        "flag_cross_currency","flag_round_amount","flag_exact_threshold",
        "flag_micro_amount","flag_cash_equivalent",
        "flag_structuring","flag_velocity_outgoing","flag_counterparty_diversity",
        "flag_rapid_transactions",
        "risk_prob","risk_prob_adjusted","risk_multiplier",
        "lr_risk_score","rf_risk_score","if_anomaly_score","severity"
    ]
    flagged = flagged[[c for c in cols_to_show if c in flagged.columns]]

    df.to_csv(DF_PATH, index=False)
    flagged.to_csv(FLAGGED_PATH, index=False)

    print(f"\n✅ Saved flagged rows to: {os.path.abspath(FLAGGED_PATH)}")
    print(f"✅ Saved full scored dataset to: {os.path.abspath(DF_PATH)}")

    safe_preview(flagged, n=10, name="Flagged (Elevated/High)")

    print(f"\n⏱ Total runtime: {round(time.time()-t0, 2)}s")


if __name__ == "__main__":
    main()